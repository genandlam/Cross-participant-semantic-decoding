{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import config, consts, paths\n",
    "from decoding.GPT import GPT\n",
    "from decoding.utils_stim import get_stim\n",
    "from decoding.utils_resp import get_resp_test\n",
    "from decoding.StimulusModel import LMFeatures\n",
    "from encoding.ridge import ridge, bootstrap_ridge\n",
    "from utils import flatten_list, save_data\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15523a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject='UTS03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e381f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_serializable(downsampled_feat):\n",
    "    \"\"\"Convert downsampled feature dictionary to a serializable format.\"\"\"\n",
    "    \n",
    "    serializable_dict = downsampled_feat.tolist()\n",
    "\n",
    "    return serializable_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97eefa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(stories, subject):\n",
    "\t\"\"\"Get the subject\"s fMRI response for stories.\"\"\"\n",
    "\t#main_path = pathlib.Path(__file__).parent.parent.resolve()\n",
    "\tsubject_dir = os.path.join(config.DATA_DIR, \"derivative/preprocessed_data/%s\" % subject)\n",
    "\tbase = subject_dir\n",
    "\tresp = []\n",
    "\trun_on_set = []\n",
    "\tfor story in stories:\n",
    "\t\tresp_path = os.path.join(base, \"%s.hf5\" % story)\n",
    "\t\thf = h5py.File(resp_path, \"r\")\n",
    "\t\tresp.extend(hf[\"data\"][:])\n",
    "\t\tif not run_on_set:\n",
    "\t\t\trun_on_set.append(hf[\"data\"][:].shape[0])\n",
    "\t\telse:\n",
    "\t\t\trun_on_set.append(run_on_set[-1]+hf[\"data\"][:].shape[0])\n",
    "\t\t#print(hf[\"data\"][:].shape[0], \"for story:\", story)\n",
    "\t\thf.close()\n",
    "\treturn np.array(resp), run_on_set[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a282455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gpt\n",
    "stories = flatten_list(consts.STORIES)\n",
    "stories_t =consts.STORIES_test\n",
    "with open(os.path.join(config.DATA_LM_DIR, \"perceived\", \"vocab.json\"), \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n",
    "gpt = GPT(path = os.path.join(config.DATA_LM_DIR, \"perceived\", \"model\"), vocab = gpt_vocab)\n",
    "features = LMFeatures(model = gpt, layer = config.GPT_LAYER, context_words = config.GPT_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccd861ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate noise model\n",
    "num_voxels = consts.NUM_VOXELS[subject]\n",
    "rstim, tr_stats, word_stats = get_stim(stories, \"story\", features)\n",
    "alphas = np.zeros(num_voxels)\n",
    "bscorrs = np.zeros([len(config.ALPHAS), num_voxels, config.NBOOTS])\n",
    "voxels = np.sort(np.argsort(bscorrs)[-config.VOXELS:])\n",
    "#splits = np.array_split(range(num_voxels), 2)\n",
    "weights = np.zeros([rstim.shape[1], num_voxels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cca95d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1869, 3072)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rstim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0c97876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 95556)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zRresp_t = get_resp_test(subject)\n",
    "zRresp_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23768dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1869, 95556)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zRresp,run_on_set = get_response(stories, subject)\n",
    "zRresp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ce840ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[343, 698, 1065, 1465]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_on_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dd5d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location=os.path.join(paths.EM % subject)\n",
    "#os.makedirs(save_location, exist_ok=True)\n",
    "#with open(save_location+'/run_on.json', \"w\") as file:\n",
    "#    json.dump(run_on_set,file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "927fce8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ckj24/genevieve/Cross-participant-semantic-decoding/results/encoding/UTS03'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b957ffb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1869"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(convert_to_serializable(rstim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bebc4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_location+'/features_train.json', \"w\") as file:\n",
    "        json.dump(convert_to_serializable(rstim),file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7fd45a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_location+'/fmri_train.json', \"w\") as file:\n",
    "        json.dump(convert_to_serializable(zRresp),file, indent=4)\n",
    "with open(save_location+'/features_train.json', \"w\") as file:\n",
    "        json.dump(convert_to_serializable(rstim),file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2772a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_location=os.path.join(paths.EM % subject)\n",
    "#with open(save_location+'/fmri_test.json', \"w\") as file:\n",
    "#        json.dump(convert_to_serializable(zRresp),file, indent=4)\n",
    "#with open(save_location+'/features_test.json', \"w\") as file:\n",
    "#        json.dump(convert_to_serializable(rstim),file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe157527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate noise model\n",
    "num_voxels = consts.NUM_VOXELS[subject]\n",
    "rstim, tr_stats, word_stats = get_stim(stories, \"story\", features)\n",
    "splits = np.array_split(range(num_voxels), 2)\n",
    "weights = np.zeros([rstim.shape[1], num_voxels])\n",
    "alphas = np.zeros(num_voxels)\n",
    "bscorrs = np.zeros([len(config.ALPHAS), num_voxels, config.NBOOTS])\n",
    "for split in splits:\n",
    "    rresp = get_resp(subject, stories, \"story\", voxels = split, stack = True)\n",
    "    weights[:, split], alphas[split], bscorrs[:, split, :] = bootstrap_ridge(rstim, rresp, alphas = config.ALPHAS, \n",
    "            nboots = config.NBOOTS, chunklen = config.CHUNKLEN, use_corr = False, seed = 42)        \n",
    "    del rresp\n",
    "bscorrs = bscorrs.mean(2).max(0)\n",
    "voxels = np.sort(np.argsort(bscorrs)[-config.VOXELS:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34866682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate noise model\n",
    "stim_dict = {story : get_stim([story], \"story\", features, tr_stats = tr_stats) for story in stories}\n",
    "resp_dict = get_resp(subject, stories, \"story\", voxels = voxels, stack = False)\n",
    "noise_model = np.zeros([len(voxels), len(voxels)])\n",
    "for hstory in stories:\n",
    "    tstim, hstim = np.vstack([stim_dict[tstory] for tstory in stories if tstory != hstory]), stim_dict[hstory]\n",
    "    tresp, hresp = np.vstack([resp_dict[tstory] for tstory in stories if tstory != hstory]), resp_dict[hstory]\n",
    "    bs_weights = ridge(tstim, tresp, alphas[voxels])\n",
    "    resids = hresp - hstim.dot(bs_weights)\n",
    "    bs_noise_model = resids.T.dot(resids)\n",
    "    noise_model += bs_noise_model / np.diag(bs_noise_model).mean() / len(stories)\n",
    "del stim_dict, resp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e796c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location=os.path.join(paths.EM % subject)\n",
    "em = {}\n",
    "em[\"bscorrs\"] = bscorrs\n",
    "em[\"voxels\"] = voxels\n",
    "em[\"tr_stats\"] = tr_stats\n",
    "em[\"word_stats\"] = word_stats\n",
    "em[\"stories\"] = stories\n",
    "em[\"weights\"] = weights[:, voxels]\n",
    "em[\"noise_model\"] = noise_model\n",
    "save_data(save_location, em)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri-algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
