{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b487f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from utils import flatten_list, save_data\n",
    "import config, consts, paths\n",
    "from encoding.ridge import bootstrap_ridge\n",
    "from decoding.StimulusModel import StimulusModel, get_lanczos_mat, affected_trs, LMFeatures\n",
    "from decoding.utils_stim import predict_word_rate, predict_word_times\n",
    "from decoding.utils_resp import get_resp, get_resp_test\n",
    "from utils import nsort, flatten_list\n",
    "from decoding.GPT import GPT\n",
    "from encoding.npp import zscore\n",
    "from decoding.Decoder import Decoder, Hypothesis\n",
    "from decoding.LanguageModel import LanguageModel\n",
    "from decoding.EncodingModel import EncodingModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf35979",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = flatten_list(consts.STORIES)\n",
    "stimuli = stories\n",
    "modality = \"story\"\n",
    "goal = \"UTS01\"\n",
    "exclude = None\n",
    "references = [\"UTS02\", \"UTS03\"]#, \"UTS09\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ba6b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reverse_corrs(goal, references, resp, cache = None):\n",
    "    \"\"\"select goal voxels by fitting reverse converters from references to goal\n",
    "    \"\"\"\n",
    "       \n",
    "    # fit converters from references to goal\n",
    "    rconverters = {}\n",
    "    for reference in references:\n",
    "        rvox = np.load(paths.EM % reference, allow_pickle = True).item()['voxels']\n",
    "        gresp_align = resp[goal]\n",
    "        rresp_align = resp[reference][:, rvox]\n",
    "        converter, _, _ = bootstrap_ridge(rresp_align, gresp_align, alphas = config.ALPHAS,\n",
    "                nboots = config.NBOOTS, chunklen = config.CHUNKLEN, use_corr = True, seed = 42)            \n",
    "        rconverters[reference] = (converter, rvox)        \n",
    "\n",
    "    # compare aligned responses across converters\n",
    "    stories = flatten_list(consts.STORIES[3:])\n",
    "    print(stories)\n",
    "    reverse_corrs = []\n",
    "    for story in stories:\n",
    "        rsim = []\n",
    "        for reference in references:\n",
    "            rvox = rconverters[reference][1]\n",
    "            rresp = get_resp(reference, [story], \"story\", stack = True, voxels = rvox)\n",
    "            rsim.append(zscore(rresp.dot(rconverters[reference][0])))\n",
    "        for c1 in range(len(rsim)):\n",
    "            for c2 in range(c1+1, len(rsim)):\n",
    "                reverse_corrs.append((rsim[c1] * rsim[c2]).mean(0))\n",
    "    return np.mean(reverse_corrs, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load converter data\n",
    "resp_align = {}\n",
    "for subject in consts.SUBJECTS:\n",
    "    #resp_full = get_resp(subject, stories, \"story\", stack = True)\n",
    "    resp_align[subject] = get_resp(subject, stimuli, modality, stack = True)\n",
    "reverse_corrs = get_reverse_corrs(goal, [subject for subject in consts.SUBJECTS if subject != goal], resp_align)\n",
    "if exclude is not None:\n",
    "    exclude_mask = np.load(paths.ROI % (goal, exclude))\n",
    "    reverse_corrs[exclude_mask] = -1\n",
    "gvox = nsort(np.argsort(reverse_corrs)[-15000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e5d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train converters\n",
    "converters = {}\n",
    "for reference in references:\n",
    "    rvox = np.load(paths.EM % reference, allow_pickle = True).item()['voxels']\n",
    "    gresp_align = resp_align[goal][:, gvox]\n",
    "    rresp_align = resp_align[reference][:, rvox]\n",
    "    converter, _, _ = bootstrap_ridge(gresp_align, rresp_align, alphas = config.ALPHAS, \n",
    "            nboots = config.NBOOTS, chunklen = config.CHUNKLEN, use_corr = True, seed = 42)            \n",
    "    converters[reference] = (converter, gvox, rvox)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53de3afa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/genevievelam/Documents/GitHub/Cross-participant-semantic-decoding/data_lm/decoder_vocab.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mDATA_LM_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperceived\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     gpt_vocab \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mDATA_LM_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_vocab.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     decoder_vocab \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m gpt \u001b[38;5;241m=\u001b[39m GPT(path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config\u001b[38;5;241m.\u001b[39mDATA_LM_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperceived\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m), vocab \u001b[38;5;241m=\u001b[39m gpt_vocab, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/genevievelam/Documents/GitHub/Cross-participant-semantic-decoding/data_lm/decoder_vocab.json'"
     ]
    }
   ],
   "source": [
    "# load gpt\n",
    "with open(os.path.join(config.DATA_LM_DIR, \"perceived\", \"vocab.json\"), \"r\") as f:\n",
    "    gpt_vocab = json.load(f)\n",
    "with open(os.path.join(config.DATA_LM_DIR, \"decoder_vocab.json\"), \"r\") as f:\n",
    "    decoder_vocab = json.load(f)\n",
    "gpt = GPT(path = os.path.join(config.DATA_LM_DIR, \"perceived\", \"model\"), vocab = gpt_vocab, device = \"cuda\")\n",
    "features = LMFeatures(model = gpt, layer = config.GPT_LAYER, context_words = config.GPT_WORDS)\n",
    "lm = LanguageModel(gpt, decoder_vocab, nuc_mass = config.LM_MASS, nuc_ratio = config.LM_RATIO)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae844ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load responses\n",
    "gresp = get_resp_test(goal, repeat = \"first\")\n",
    "\n",
    "# load models\n",
    "em = {}\n",
    "wr_data = []\n",
    "for reference in references:\n",
    "    wr_data.append(np.load(paths.WR % reference, allow_pickle = True).item())\n",
    "    em_data = np.load(paths.EM % reference, allow_pickle = True).item()\n",
    "    converter, gvox, rvox = converters[reference]\n",
    "    rresp = np.nan_to_num(zscore(gresp[:, gvox].dot(converter)))\n",
    "    em[reference] = EncodingModel(rresp, em_data[\"weights\"], em_data[\"noise_model\"], device = \"cuda\")\n",
    "    em[reference].set_shrinkage(config.NM_ALPHA)\n",
    "    tr_stats, word_stats = np.array(em_data[\"tr_stats\"]), em_data[\"word_stats\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dbf7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict word times\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"ROIs\", f\"{goal}.json\"), \"r\") as f:\n",
    "    roi_vox = json.load(f)\n",
    "starttime = -10\n",
    "word_rate = predict_word_rate(gresp, roi_vox, wr_data)\n",
    "word_times, tr_times = predict_word_times(word_rate, gresp, starttime = starttime)\n",
    "lanczos_mat = get_lanczos_mat(word_times, tr_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263968bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode responses\n",
    "decoder = Decoder(word_times, config.WIDTH)\n",
    "sm = StimulusModel(lanczos_mat, em_data[\"tr_stats\"], em_data[\"word_stats\"][0], device = \"cuda\")\n",
    "for sample_index in range(len(word_times)):\n",
    "    trs = affected_trs(decoder.first_difference(), sample_index, lanczos_mat)\n",
    "    ncontext = decoder.time_window(sample_index, config.LM_TIME, floor = 5)\n",
    "    beam_nucs = lm.beam_propose(decoder.beam, ncontext)\n",
    "    for c, (hyp, nextensions) in enumerate(decoder.get_hypotheses()):\n",
    "        nuc, logprobs = beam_nucs[c]\n",
    "        if len(nuc) < 1: continue\n",
    "        extend_words = [hyp.words + [x] for x in nuc]\n",
    "        extend_embs = list(features.extend(extend_words))\n",
    "        stim = sm.make_variants(sample_index, hyp.embs, extend_embs, trs)\n",
    "        likelihoods = {}\n",
    "        for reference in references:\n",
    "            likelihoods[reference] = em[reference].prs(stim, trs)\n",
    "        mean_likelihoods = np.sum([likelihoods[reference] for reference in  references], axis = 0)\n",
    "        local_extensions = [Hypothesis(parent = hyp, extension = x) for x in zip(nuc, logprobs, extend_embs)]\n",
    "        decoder.add_extensions(local_extensions, mean_likelihoods, nextensions)\n",
    "    decoder.extend(verbose = (sample_index % 20 == 0))\n",
    "#decoder.save(args.save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naturalistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
