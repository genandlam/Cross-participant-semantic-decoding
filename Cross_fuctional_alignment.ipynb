{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b487f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/naturalistic/lib/python3.12/site-packages/torch/__init__.py:1264: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:436.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from utils import flatten_list, save_data\n",
    "import config, consts, paths\n",
    "from encoding.ridge import bootstrap_ridge\n",
    "from decoding.StimulusModel import StimulusModel, get_lanczos_mat, affected_trs, LMFeatures\n",
    "from decoding.utils_stim import predict_word_rate, predict_word_times\n",
    "from decoding.utils_resp import get_resp, get_resp_test\n",
    "from utils import nsort, flatten_list\n",
    "from decoding.GPT import GPT\n",
    "from encoding.npp import zscore\n",
    "from decoding.Decoder import Decoder, Hypothesis\n",
    "from decoding.LanguageModel import LanguageModel\n",
    "from decoding.EncodingModel import EncodingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf35979",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = flatten_list(consts.STORIES)\n",
    "stimuli = stories\n",
    "modality = \"story\"\n",
    "goal = \"UTS09\"\n",
    "exclude = None\n",
    "references = [\"UTS02\", \"UTS03\"]#, \"UTS09\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba6b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reverse_corrs(goal, references, resp, cache = None):\n",
    "    \"\"\"select goal voxels by fitting reverse converters from references to goal\n",
    "    \"\"\"\n",
    "       \n",
    "    # fit converters from references to goal\n",
    "    rconverters = {}\n",
    "    for reference in references:\n",
    "        rvox = np.load(paths.EM % reference, allow_pickle = True).item()['voxels']\n",
    "        gresp_align = resp[goal]\n",
    "        rresp_align = resp[reference][:, rvox]\n",
    "        converter, _, _ = bootstrap_ridge(rresp_align, gresp_align, alphas = config.ALPHAS,\n",
    "                nboots = config.NBOOTS, chunklen = config.CHUNKLEN, use_corr = True, seed = 42)   \n",
    "        # wt, valphas, allRcorrs         \n",
    "        rconverters[reference] = (converter, rvox)        \n",
    "\n",
    "    # compare aligned responses across converters\n",
    "    stories = flatten_list(consts.STORIES[3:])\n",
    "    print(stories)\n",
    "    reverse_corrs = []\n",
    "    for story in stories:\n",
    "        rsim = []\n",
    "        for reference in references:\n",
    "            rvox = rconverters[reference][1]\n",
    "            rresp = get_resp(reference, [story], \"story\", stack = True, voxels = rvox)\n",
    "            rsim.append(zscore(rresp.dot(rconverters[reference][0])))\n",
    "        for c1 in range(len(rsim)):\n",
    "            for c2 in range(c1+1, len(rsim)):\n",
    "                reverse_corrs.append((rsim[c1] * rsim[c2]).mean(0))\n",
    "    return np.mean(reverse_corrs, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load converter data\n",
    "resp_align = {}\n",
    "for subject in consts.SUBJECTS:\n",
    "    #resp_full = get_resp(subject, stories, \"story\", stack = True)\n",
    "    resp_align[subject] = get_resp(subject, stimuli, modality, stack = True)\n",
    "reverse_corrs = get_reverse_corrs(goal, [subject for subject in consts.SUBJECTS if subject != goal], resp_align)\n",
    "if exclude is not None:\n",
    "    exclude_mask = np.load(paths.ROI % (goal, exclude))\n",
    "    reverse_corrs[exclude_mask] = -1\n",
    "gvox = nsort(np.argsort(reverse_corrs)[-15000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e5d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train converters\n",
    "converters = {}\n",
    "for reference in references:\n",
    "    rvox = np.load(paths.EM % reference, allow_pickle = True).item()['voxels']\n",
    "    gresp_align = resp_align[goal][:, gvox]\n",
    "    rresp_align = resp_align[reference][:, rvox]\n",
    "    converter, _, _ = bootstrap_ridge(gresp_align, rresp_align, alphas = config.ALPHAS, \n",
    "            nboots = config.NBOOTS, chunklen = config.CHUNKLEN, use_corr = True, seed = 42)\n",
    "                \n",
    "    converters[reference] = (converter, gvox, rvox)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae844ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load responses\n",
    "gresp = get_resp_test(goal, repeat = \"first\")\n",
    "\n",
    "# load models\n",
    "em = {}\n",
    "wr_data = []\n",
    "for reference in references:\n",
    "    wr_data.append(np.load(paths.WR % reference, allow_pickle = True).item())\n",
    "    em_data = np.load(paths.EM % reference, allow_pickle = True).item()\n",
    "    converter, gvox, rvox = converters[reference]\n",
    "    rresp = np.nan_to_num(zscore(gresp[:, gvox].dot(converter)))\n",
    "    em[reference] = EncodingModel(rresp, em_data[\"weights\"], em_data[\"noise_model\"], device = \"cuda\")\n",
    "    em[reference].set_shrinkage(config.NM_ALPHA)\n",
    "    tr_stats, word_stats = np.array(em_data[\"tr_stats\"]), em_data[\"word_stats\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dbf7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict word times\n",
    "with open(os.path.join(config.DATA_TRAIN_DIR, \"ROIs\", f\"{goal}.json\"), \"r\") as f:\n",
    "    roi_vox = json.load(f)\n",
    "starttime = -10\n",
    "word_rate = predict_word_rate(gresp, roi_vox, wr_data)\n",
    "word_times, tr_times = predict_word_times(word_rate, gresp, starttime = starttime)\n",
    "lanczos_mat = get_lanczos_mat(word_times, tr_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc65a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location=os.path.join(paths.EM % subject)\n",
    "em = {}\n",
    "em[\"starttime\"] = starttime\n",
    "em[\"word_times\"] = word_times\n",
    "em[\"tr_times\"] = tr_times\n",
    "em[\"stories\"] = stories\n",
    "em[\"lanczos_mat\"] = lanczos_mat\n",
    "#save_data(save_location, em)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naturalistic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
